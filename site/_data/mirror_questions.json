[
  {
    "id": 1,
    "pair": 1,
    "pairName": "What you think you're doing / What the system is doing",
    "text": "When you use AI to help with a task, what do you think is happening?",
    "options": [
      { "label": "The AI is serving my goals", "scores": { "filtering": 0 } },
      { "label": "We're collaborating", "scores": { "filtering": 1 } },
      { "label": "All of the above", "scores": { "filtering": 2 } },
      { "label": "The AI is shaping my output / I'm training the AI", "scores": { "filtering": 3 } }
    ],
    "inversion": null
  },
  {
    "id": 2,
    "pair": 1,
    "pairName": "What you think you're doing / What the system is doing",
    "text": "When an AI tool gets better at predicting what you want, how do you feel?",
    "options": [
      { "label": "Pleased—it's more useful", "scores": { "filtering": 0 } },
      { "label": "I hadn't noticed", "scores": { "filtering": 1 } },
      { "label": "Slightly uneasy", "scores": { "filtering": 2 } },
      { "label": "Concerned", "scores": { "filtering": 3 } }
    ],
    "inversion": "When you use AI, you're also: generating training data, providing feedback on outputs, normalizing the tool's patterns, creating dependency, and accepting its constraints as defaults. The question isn't whether these are bad. It's whether you noticed them.\n\nAn AI that predicts you well has modeled you well. That model exists on someone else's server, informs someone else's business decisions, and nudges you toward predictable patterns because predictable users are valuable users. 'Personalization' and 'control' are the same mechanism, described from different positions."
  },
  {
    "id": 3,
    "pair": 2,
    "pairName": "What you think you're seeing / What you're not seeing",
    "text": "When you scroll through a feed, what determines what you see?",
    "options": [
      { "label": "My choices and preferences", "scores": { "filtering": 0 } },
      { "label": "I don't know", "scores": { "filtering": 1 } },
      { "label": "A mix of both", "scores": { "filtering": 2 } },
      { "label": "The algorithm's predictions", "scores": { "filtering": 3 } }
    ],
    "inversion": null
  },
  {
    "id": 4,
    "pair": 2,
    "pairName": "What you think you're seeing / What you're not seeing",
    "text": "Have you ever tried to see what content is being filtered from you?",
    "options": [
      { "label": "I assumed nothing was filtered", "scores": { "filtering": 0 } },
      { "label": "I wouldn't know how", "scores": { "filtering": 1 } },
      { "label": "Yes, occasionally", "scores": { "filtering": 2 } },
      { "label": "Yes, regularly", "scores": { "filtering": 3 } }
    ],
    "inversion": "What you see is the result of billions of micro-decisions by an optimization system. But there's a much larger set of things you *don't* see—filtered out because they wouldn't engage you, weren't 'safe,' or came from sources without sufficient reach. The feed isn't a window. It's a wall with carefully chosen holes.\n\nThe design makes it nearly impossible to see what you're not seeing. There's no 'filtered' folder. No transparency report. The absence is invisible by design. Not because anyone is hiding something—but because showing you what you're missing would reduce engagement. The system optimizes for satisfaction, not completeness."
  },
  {
    "id": 5,
    "pair": 3,
    "pairName": "What you think you chose / What was chosen for you",
    "text": "When you make choices online—what to click, read, buy, believe—how free do you feel?",
    "options": [
      { "label": "Completely free", "scores": { "agency": 0 } },
      { "label": "Mostly free, but influenced", "scores": { "agency": 1 } },
      { "label": "Significantly shaped by what I'm shown", "scores": { "agency": 2 } },
      { "label": "I'm not sure anymore", "scores": { "agency": 3 } }
    ],
    "inversion": null
  },
  {
    "id": 6,
    "pair": 3,
    "pairName": "What you think you chose / What was chosen for you",
    "text": "When AI 'recommends' something, do you treat it as a suggestion or a default?",
    "options": [
      { "label": "Often a default—I usually accept", "scores": { "agency": 0 } },
      { "label": "I hadn't distinguished between these", "scores": { "agency": 1 } },
      { "label": "Somewhere in between", "scores": { "agency": 2 } },
      { "label": "Suggestion—I consider many options", "scores": { "agency": 3 } }
    ],
    "inversion": "Choice architecture is a well-studied field. The options you see, the order they appear, the defaults that are pre-selected—all of these shape choices more than conscious deliberation. AI supercharges this: it learns *which* architectures work on *you*. Your choices are still yours. But the menu was written for you, personally, by a system that knows what you'll order.\n\nDefaults are powerful. Most people accept defaults. AI recommendations are framed as 'suggestions,' but they function as defaults: they're the easiest path, and deviation requires effort. Over time, the path of least resistance becomes the only path you remember exists."
  },
  {
    "id": 7,
    "pair": 4,
    "pairName": "What you think is neutral / What has embedded values",
    "text": "Do you believe the AI tools you use have political or social values embedded in them?",
    "options": [
      { "label": "No, they're neutral tools", "scores": { "agency": 0 } },
      { "label": "The question doesn't make sense to me", "scores": { "agency": 1 } },
      { "label": "They try to be neutral but have some bias", "scores": { "agency": 2 } },
      { "label": "They have significant embedded values", "scores": { "agency": 3 } }
    ],
    "inversion": null
  },
  {
    "id": 8,
    "pair": 4,
    "pairName": "What you think is neutral / What has embedded values",
    "text": "When AI refuses to do something or adds caveats, what do you think is happening?",
    "options": [
      { "label": "Safety guardrails protecting me", "scores": { "agency": 0 } },
      { "label": "I hadn't thought about it", "scores": { "agency": 1 } },
      { "label": "A mix of safety and corporate decisions", "scores": { "agency": 2 } },
      { "label": "Value judgments I didn't ask for", "scores": { "agency": 3 } }
    ],
    "inversion": "Every choice in AI design is a value choice. What data to train on. What outputs to discourage. How to handle controversy. What 'helpful' means. What 'harmful' means. The engineers making these choices are humans with worldviews. 'Neutrality' isn't absence of values—it's *invisibility* of values. The tool feels neutral because you can't see the choices that made it.\n\nAI 'safety' is a set of decisions about what should and shouldn't be easy to do or say. Some of these decisions you'd agree with. Some you wouldn't—if you knew about them. The boundaries aren't laws of nature. They're policies, set by companies, enforced by code, invisible until you bump into them. And the enforcement is quiet: not a block, but a nudge away from certain thoughts."
  },
  {
    "id": 9,
    "pair": 5,
    "pairName": "What you think is personal / What is systemic",
    "text": "When you use AI in ways that concern you slightly, how do you think about it?",
    "options": [
      { "label": "It's my personal choice, small impact", "scores": { "systemic": 0 } },
      { "label": "I don't use AI in ways that concern me", "scores": { "systemic": 1 } },
      { "label": "I'm part of a larger pattern but can't change it", "scores": { "systemic": 2 } },
      { "label": "My usage contributes to systemic effects", "scores": { "systemic": 3 } }
    ],
    "inversion": null
  },
  {
    "id": 10,
    "pair": 5,
    "pairName": "What you think is personal / What is systemic",
    "text": "If millions of people use AI the same way you do, what world does that create?",
    "options": [
      { "label": "A better world—AI helps us", "scores": { "systemic": 0 } },
      { "label": "Roughly the same as now", "scores": { "systemic": 1 } },
      { "label": "A more homogenized, filtered world", "scores": { "systemic": 2 } },
      { "label": "A more surveilled, controlled world", "scores": { "systemic": 3 } }
    ],
    "inversion": "Individual choices aggregate. This is the hardest thing to see, and the easiest to dismiss. 'I'm just one person.' But AI systems learn from millions of 'just one person' interactions. Your acceptance of a suggestion, your engagement with content, your willingness to let AI handle a task—each one is a vote in a system that's always holding elections.\n\nThis is the question. Not 'is AI bad' but 'what does AI at scale do to thought, expression, and power?' The answer isn't predetermined. But it depends on millions of people asking the question. The greatest success of these systems would be making the question feel irrelevant—just the way things are, nothing to see here, too convenient to question."
  }
]
